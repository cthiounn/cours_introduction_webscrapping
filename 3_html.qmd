---
title: "Bases pour le Webscraping"
author: "cthiounn"
date: "2024-05-20"
format: html
---

## Comprendre les technologies d'Internet

Pour faire du Webscraping, il est primordiable de comprendre les bases du Web.
Il s'agit de comprendre les manières d'interagir avec les sites et API, avec notamment le [protocole/langage HTTP](https://fr.wikipedia.org/wiki/Hypertext_Transfer_Protocol).
et de connaître les formats de données ([HTML](https://fr.wikipedia.org/wiki/Hypertext_Markup_Language), XML, formats de fichiers, etc)

## Naviguer sur le Web

Pour naviguer sur le Web, notre internaute Lambda utilise son navigateur Web préférée (Edge, Chrome, Mozilla, Opéra, etc.) en tapant le nom du site dans la barre de navigation (1).
Le navigateur affiche la page d'accueil (2). Lambda clique sur un lien (3) et le navigateur lui affiche la nouvelle page (4).

Décortiquons ce qui s'est passé (en omettant des parties techniques comme DNS, les IP et le routage des informations) :

(1) En tapant le nom du site, Lambda dit à son navigateur qu'il souhaite accèder au site (admettons www.insee.fr/) . Le navigateur utilise le langage HTTP au serveur de l'Insee pour demander la page d'accueil, le navigateur envoie la demande "GET /"
(2) Le serveur de l'Insee répond avec la page HTML d'accueil. Le navigateur affiche à l'utilisateur Lambda la page HTML (qui est juste une suite de caractères comprennant des informations entre des balises standardisées <html><header></header><body></body></html>)
(3) Lambda clique sur un lien (e.g. \<a href="/concours/"> Ma page Concours</a>), le navigateur comprends qu'il doit refaire une requête HTTP "GET /concours/"
(4) Le serveur lui répond avec une nouvelle page HTML. (et ainsi de suite...)

## Le protocole HTTP(S) et API

Pour interagir avec un site Web (serveur HTTP), il y a un certain nombre de commandes que nous pouvons lui soumettre, aussi appelée [verbe HTTP](https://fr.wikipedia.org/wiki/Hypertext_Transfer_Protocol#M%C3%A9thodes). Voici les plus fréquents :

* **GET** (=récupérer une information, en général une page HTML ou un fichier)
* **POST** (=soumettre de la donnée et récupérer éventuellement une information, obligatoirement le cas dans un formulaire sensible)

Dans le cas des API, si vous avez les droits sur l'API :

* **PUT** (=mettre à jour une information)
* **DELETE** (=supprimer une information)

Pour le Webscraping, nous allons principalement soumettre des requêtes GET et POST (tout comme dans la navigation Web classique)

Le [protocole HTTPS](https://fr.wikipedia.org/wiki/Hypertext_Transfer_Protocol_Secure) (SSL en plus de HTTP) est juste l'ajout d'un chiffrement de toutes les communications, notamment obligatoire si vous soumettez un mot de passe ou une information sensible.
Sans le chiffrement, tout est public et visible à qui se donne la peine de vouloir espionner.
Voyez le chiffrement comme les vitres teintées d'un moyen de locomotion entre deux sites sensibles, dans un bon film d'espionnage !

Les différents sites sont navigables à travers des pages HTML (section suivante) ou renvoient des informations par des API, service renvoyant de la donnée à qui sait la manipuler.

## La page HTML

Pour la navigation Web classique, l'information renvoyée par le serveur est structurée au format [HTML](https://fr.wikipedia.org/wiki/Hypertext_Markup_Language) : il y a notamment un titre principal et un contenu hierarchisé.
Le serveur renvoie un ensemble de caractères toujours structurée de la même façon de manière macro :

* balise \<html> \</html> pour délimiter tout le périmètre
* à l'intérieur, redecoupage en balise \<head>\</head> || \<body>\</body>
* les données intéressantes sont généralement dans le body (corps du contenu)
* dans le corps du contenu \<body>, utilisation d'un vocabulaire de balise 

Exemples de balises les plus fréquentes :

* \<p> : paragraphe
* \<h1> \<h2> ... : titre 
* \<a> : lien HTTP 
* \<div> : bloc de contenu (pas de signification sémantique, utilisé pour les scripts ou le CSS)
* \<section> : ensemble de contenu (signification sémantique, comme un chapitre d'un livre)
* \<button> : bouton
* \<form> : formulaire

Cette structuration permet de questionner la page (est-ce qu'il y a des liens HTTP présents sur la page ? si oui, renvoie les moi) et récupérer les informations associées.

## Au delà de HTML : CSS et Javascript

La librairie CSS permet d'habiller le site de couleur et de mieux agencer l'information. Le CSS n'est pas utile pour le Webscraping mais seulement agréable à l'oeil des internautes.
Cependant, les scripts Javascript permettent d'ajouter du dynamisme au site, en rajoutant des effets conditionnels, de charger les données a posteriori.
Le chargement de données a posteriori par des Javascript va complexifier les méthodes utilisées pour le Webscraping.